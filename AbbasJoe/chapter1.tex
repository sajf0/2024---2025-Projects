\chapter{طريقة التفاضل التربيعي}
	\pagestyle{fancy}
\section[مقدمة]{مقدمة \LR{Introduction}}

يُعد التفاضل التربيعي  \LR{(Differential Quadrature)} من الأساليب العددية الفعالة لتقريب المشتقات، حيث يعتمد على مبدأ تمثيل المشتقة كمجموعٍ موزونٍ لقيم الدالة عند نقاط محددة. يوفر هذا الأسلوب دقة عالية عند حل المعادلات التفاضلية الجزئية، مما يجعله بديلاً قوياً للطُرق التقليدية مثل الفروق المحدودة والعناصر المحدودة.

يتناول هذا الفصل الأسس الرياضية التي يقوم عليها التفاضل التربيعي، حيث يُستعرض عددٌ من الطرائق المختلفة لاشتقاق الصيغ الأساسية لتقريب المشتقة. كما يتم التطرق إلى كيفية حساب معاملات الوزن للمشتقات من الرتبة الأولى، والتي تُعد عنصراً جوهرياً في دقة الطريقة. بالإضافة إلى ذلك، يناقش الفصل أنواع نقاط الشبكة المستخدمة في التقريب، والتي تلعب دوراً مهماً في تحسين استقرار الحسابات وتقليل الخطأ العددي.

\section[صيغ التفاضل التربيعي]{صيغ التفاضل التربيعي \LR{Differential Quadrature Formulas}}

تتلخص فكرة عمل طريقة التفاضل التربيعي في تقريب المشتقات الجزئية (أو الاعتيادية) بواسطة مجموع الاوزان لمتغيرات الدالة المعطاة في كل نقاط الشبكة وضمن المجال المحدد لحساب قيم الدالة. لتكن الدالة $y=f(x)$ معرفة على الفترة $[a,b]$ حيث $a,b$ ثوابت ولنفرض أن المنطقة قسمت إلى $N$ من النقاط كما موضح في الشكل \ref{fig:numberline}\cite{chang_shu}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\draw[latex-latex] (-3.5,0) -- (3.5,0) ; %edit here for the axis
		\foreach \x in  {-3,-2,-1,0,1,2,3} % edit here for the vertical lines
		\draw[shift={(\x,0)},color=black] (0pt,3pt) -- (0pt,-3pt);
		\draw[shift={(-3,0)}] (0pt,0pt) -- (0pt,-3pt) node [below] {$x_1$};
		\draw[shift={(0,0)}] (0pt,0pt) -- (0pt,-3pt) node [below] {$x_i$};
		\draw[shift={(3,0)}] (0pt,0pt) -- (0pt,-3pt) node [below] {$x_N$};
	\end{tikzpicture}
	
	\caption{تقسيم الفترة $[a,b]$}
	\label{fig:numberline}
\end{figure}

لذلك يكون تقريب المشتقة الأولى و الثانية للدالة $f(x)$ عند النقطة $x_i$ بالشكل
\begin{align}
	\subs{\frac{dy}{dx}}_{x=x_i}=\sum_{j=1}^{N}a_{ij}^{(1)}f(x_j) \label{eq:firstorderDQM}\\
	\subs{\frac{d^2y}{dx^2}}_{x=x_i}=\sum_{j=1}^{N}a_{ij}^{(2)}f(x_j) \label{eq:secondorderDQM}
\end{align}
وبشكل عام
\begin{equation}
	\label{eq:DQM_formula}
	\subs{\frac{d^ry}{dx^r}}_{x=x_i}=\sum_{j=1}^{N}a_{ij}^{(r)}f(x_j)
\end{equation}
حيث $a_{ij}^{(r)}$ تمثل معاملات الوزن من الرتبة $r^{th}$ وسنوضح في البند القادم كيف يتم حساب معاملات الوزن وبيان دورها في تحديد دقة الحلول الناتجة من استعمال طريقة التفاضل التربيعي.

\section[معاملات الوزن من الرتبة الأولى]{معاملات الوزن من الرتبة الأولى \LR{Weight Coefficients of First Order}}

أن تحديد نقاط الشبكة و معاملات الوزن هما عاملان مهمان في تطبيق صيغ التفاضل التربيعي \eqref{eq:DQM_formula} وأن لمعاملات الوزن دوراً رئيسياً و مهماً في طريقة التفاضل التربيعي و تعد أحد مفاتيح هذه الطريقة لما لها من أهمية في التأثير على دقة الحلول العددية. الآن سوف نتطرق إلى طرق حساب معاملات الوزن من الرتبة الأولى


\subsection[طريقة بيلمان الأولى]{طريقة بيلمان الأولى \cite{Bellman} \LR{(Billman's First Approach)}}

في هذه الطريقة استخدم بيلمان دوال الاختبار التالية للحصول على معاملات الوزن على الشكل
\begin{equation}
	\label{test_function_1}
	g_k(x)=x^k,\quad k =0,1,2,\dots,N-1
\end{equation}
من الواضح أن معادلة \eqref{test_function_1} تعطي $N$ من دوال الإختبار. معاملات الوزن $a_{ij}^{(1)}$ في \eqref{eq:DQM_formula} ، $i$ و $j$ تأخذ قيم من $1$ إلى $N$ وبالتالي مجموع معاملات الوزن هو $N\times N$. بتطبيق دوال الإختبار على نقاط الشبكة $x_1,x_2,\dots,x_N$ ، نتيجة لذلك نحصل على $N\times N$ من المعادلات

\begin{align}
	\label{equations_system}
	&\sum_{j=1}^{N}a_{ij}^{(1)}=0\notag\\
	&\sum_{j=1}^{N}a_{ij}^{(1)}\,x_j=1\\
	&\sum_{j=1}^{N}a_{ij}^{(1)}\,x_j^k=k\,x_i^{k-1},\quad k=2,3,\dots,N-1\notag
\end{align}
لكل $i=1,2,\dots,N$. نظام المعادلات في \eqref{equations_system} يملك حل وحيد لأن مصفوفة النظام تأخذ شكل \\\textbf{Vandermonde}. لسوء الحظ عندما تكون $N$ كبيرة يصعب إيجاد حل لهذا النظام لهذا يتم إختيار قيم صغيرة إلى $N$ (أقل من $13$).

\begin{note}
	لا توجد أي قيود على اختيار نقاط الشبكة $x_i$ في طريقة بيلمان لحساب معاملات الوزن من الرتبة الأولى.
\end{note}

\subsection[طريقة بيلمان الثانية]{طريقة بيلمان الثانية \cite{Bellman} \LR{(Billman's Second Approach)}}

في هذه الطريقة أستخدم بيلمان دوال الإختبار التالية للحصول على معاملات الوزن
\begin{equation}
	\label{test_function_2}
	g_k(x)=\frac{L_N(x)}{(x-x_k)L_N^{(1)}(x)},\quad k=1,2,\dots,N
\end{equation}
حيث $L_N(x)$ هي متعددة حدود ليجندر من الدرجة $N$ و $L_N^{(1)}(x)$ هي المشتقة الأولى إلى $L_N(x)$. يتم في هذه الطريقة إختيار نقاط الشبكة $x_1,x_2,\dots,x_N$ لتكون جذور متعددة حدود ليجندر المُزاحة إلى الفترة $[0,1]$. وبتطبيق دوال الإختبار في \eqref{test_function_2} على نقاط الشبكة. بيلمان توصل إلى أن صياغة جبرية بسيطة لحساب معاملات الوزن $a_{ij}$.
\begin{equation}
	\label{bilman2_equations}
	\begin{aligned}
		&a_{ij}=\frac{L_N^{(1)}(x_i)}{(x_i-x_j)L_N^{(1)}(x_j)},\quad i\neq j\\[10pt]
		& a_{ii}=\frac{1-2x_i}{2x_i(x_i-1)}   
	\end{aligned}
\end{equation}
رغم هذه البساطة إلا أن هذه الطريقة لبست بمرونة الطريقة الأولى والسبب يعود إلى إختيار نقاط الشبكة $x_1,x_2,\dots,x_N$ حيث لا نستطيع تحديدها بشكل إختياري ، بدلاً من ذلك يتم اختيارها كجذور متعددة حدود ليجندر من الدرجة $N$. لهذا السبب فإن الطريقة الأولى تُفَضّل في التطبيقات العملية.

\begin{note}
	أن متعددات حدود ليجندر المُزاحة إلى الفترة $[0,1]$ تعطى بالصيغة 
	\begin{equation*}
		L_N^{*}(x)=\sum_{k=0}^{N}(-1)^{N+k}\frac{(N+k)!}{(N-k)!(k!)^2}\,x^k
	\end{equation*}
	أول خمس متعددات حدود ليجندر هي
	\begin{align*}
		&L_0^{*}(x)=1\\
		&L_1^{*}(x)=2x-1\\
		&L_2^{*}(x)=6x^2-6x+1\\
		&L_3^{*}(x)=20x^3-30x^2+12x-1\\
		&L_4^{*}(x)=70x^4-140x^3+90x^2-20x+1
	\end{align*}
\end{note}

\subsection[طريقة كوان و جانك]{طريقة كوان و جانك \cite{Quan} \LR{(Quan \& Chang's Approach)}}

لتطوير طُرق بيلمان في حساب معاملات الوزن ، العديد من المحاولات تمت بواسطة العديد من الباحثين. واحدة من أكثر الطرق فائدة هي الطريقة المقدمة من الباحثين كوان و جانك. حيث استعملا متعددات حدود لاكرانج كدوال إختيار
\begin{equation}
	\label{test_function_3}
	g_k(x)=\frac{M(x)}{(x-x_k)M^{(1)}(x_k)},\quad k=1,2,\dots,N
\end{equation}
حيث
\begin{align*}
	&M(x)=(x-x_1)(x-x_2)\cdots(x-x_N)\\
	&M^{(1)}(x_i)=\prod_{k=1,k\neq i}^{N}(x_i-x_k)
\end{align*}
وبتطبيق هذه الدوال على $N$ من نقاط الشبكة ، نحصل على الصيغ الجبرية لحساب معاملات الوزن $a_{ij}^{(1)}$.
\begin{equation}
	\label{quan_chang_equations}
	\begin{aligned}
		& a_{ij}^{(1)}=\frac{1}{x_j-x_i}\prod_{k=1,k\neq i,j}^{N}\frac{x_i-x_k}{x_j-x_k},\quad i\neq j \\
		& a_{ii}^{(1)}=\sum_{k=1,k\neq i}^{N}\frac{1}{x_i-x_k}
	\end{aligned}
\end{equation}
ومن المهم معرفة أنه لا توجد أية قيود على اختيار نقاط الشبكة في هذه الطريقة

\begin{note}
	طريقة كوان و جانك مكافئة لطريقة بيلمان الأولى لهذا هنا أيضاً لا توجد قيود في اختيار نقاط الشبكة $x_i$.
\end{note}

\section[معاملات الوزن من الرتبة الثانية]{معاملات الوزن من الرتبة الثانية \LR{Weight Coefficients of Second Order}}

في هذا البند سوف نتعرف على طرق حساب معاملات الوزن من الرتبة الثانية حيث
\begin{equation}
	\label{eq:DQM_formula_order2}
	\subs{\frac{d^2f}{dx^2}}_{x=x_i}=\sum_{j=1}^{N}a_{ij}^{(2)}f(x_j)
\end{equation}
حيث $a_{ij}^{(2)}$ هي معاملات الوزن من الرتبة الثانية.

\subsection[طريقة شو العامة]{طريقة شو العامة \cite{chang_shu} \en{(Shu's General Approach)}}
بإستخدام تقريب متعددات الحدود و فضاء المتجهات توصل شو إلى صياغة لمعاملات الوزن من الرتبة الثانية ، كما يلي
\begin{equation}
	\label{eq:shus_equations}
	a_{ij}^{(2)}=2a_{ij}^{(1)}\pbracket{a_{ii}^{(1)}-\frac{1}{x_i-x_j}},\quad i\not=j
\end{equation}
حيث نلاحظ من \eqref{eq:shus_equations} إذا كانت $i\neq j$ فإن $a_{ij}^{(2)}$ يمكن أن تحسب بسهولة. يمكن تطبيق نظام المعادلات في \eqref{equations_system} لـــ $k=1$ ، نحصل على
\[
\sum_{j=1}^{N}a_{ij}^{(2)}=0\Longrightarrow a_{ii}^{(2)}=-\sum_{j=1,i\neq j}^{N}a_{ij}^{(2)}
\]


\subsection[طريقة ضرب المصفوفات]{طريقة ضرب المصفوفات \cite{chang_shu} \LR{(Matrix Multiplication Method)}}
بما ان معاملات الوزن تمثل لنا مصفوفة مربعة ذات حجم $N$ يمكن الاستفادة من معلوماتنا من التفاضل والجبر الخطي للتوصل الى صيغة لحساب معاملات الوزن من الرتبة الثانية ، اولاً من تعريف المؤثر التفاضلي
\[
\der{^2f}{x^2}=\der{}{x}\pbracket{\der{f}{x}}
\]
بتطبيق طريقة التفاضل التربيعي على الطرفين ، نحصل على
\begin{align*}
	\sum_{j=1}^{N}a_{ij}^{(2)}\cdot f(x_j)&=\sum_{k=1}^{N}a_{ik}^{(1)}\cdot \subs{\der{f}{x}}_{x_k}\\
	&=\sum_{k=1}^{N}a_{ik}^{(1)}\sum_{j=1}^{N}a_{kj}^{(1)}\cdot f(x_j)\\
	&=\sum_{j=1}^{N}\sbracket{\sum_{k=1}^{N}a_{ik}^{(1)}\cdot a_{kj}^{(1)}}\cdot f(x_j)
\end{align*}
وبمقارنة الطرفين نحصل على
\begin{equation}
	a_{ij}^{(2)}=\sum_{k=1}^{N}a_{ik}^{(1)}\cdot a_{kj}^{(1)}
\end{equation}
وبلغة المصفوفات هذا يعني
\begin{equation}
	\label{second_order_equations}
	[a_{ij}^{(2)}]=[a_{ij}^{(1)}]\times[a_{ij}^{(1)}]
\end{equation}

\begin{note}
	يُفضل استخدام طريقة شو العامة لأنها تتطلب عمليات حسابية أقل من طريقة ضرب المصفوفات
\end{note}

\section[اختيار نقاط الشبكة]{اختيار نقاط الشبكة \LR{Choice of Grid Points}}

أن اختيار نقاط الشبكة واحد من العوامل المهمة التي تؤثر على دقة التقريبات الناتجة من استعمال طريقة التفاضل التربيعي. لذلك ركز العديد من الباحثين منهم \textbf{شو \en{(Shu)}} على كيفية دراسة تأثير نقاط الشبكة على دقة الحلول في هذه الطريقة. ولكن حين يمكننا التحكم بنقاط الشبكة ففي طريقة بيلمان الثانية لا يمكننا ذلك.

\subsection[النقاط متساوية الأبعاد]{النقاط متساوية الأبعاد \LR{(Equally Spaced Grid Points)}}
تكون على الشكل
\begin{equation}
	\label{eq:equally_spaced_points}
	x_i=a+\frac{b-a}{N-1}(i-1),\quad i=1,2,\dots,N
\end{equation}
هذا النوع من النقاط كان قيد الإستعمال من قبل الكثير من الباحثين لبساطتها وملائمتها في حل الكثير من المسائل

\subsection[نقاط شيبيشيف-كاوس-لوباتو]{نقاط شيبيشيف-كاوس-لوباتو \LR{(Chebyshev-Gauss-Lobatto Points)}}
ويطلق عليها اختصاراً \textbf{نقاط لوباتو \en{(Lobatto Points)}}
\begin{equation}
	\label{eq:lobatto-points}
	x_i=a+\frac{1}{2}\sbracket{1-\cos\pbracket{\frac{i-1}{N-1}}\pi}(b-a),\quad i=1,2,\dots,N
\end{equation}
وتسمى في بعض الأحيان النقاط غير متساوية الأبعاد \en{(Unequally Spaced Points)}. وقد أثبت العديد من الباحثين بأن هذا النوع من النقاط يعطي نتائج أكثر دقة من النقاط متساوية الأبعاد.




